{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM 파이프라인 구축하기\n",
    "\n",
    "## LLM 파이프라인 구축하는 이유\n",
    "\n",
    "### 1. 성능 향상\n",
    "그냥 채팅 완성 기능만 쓰면 사용자한테 별 가치 없음. 표준 GPT랑 다르게 해서 성능 확실히 올려야 함.\n",
    "\n",
    "### 2. 작업별 정확성\n",
    "데이터베이스랑 연결하거나 더 깊이 있는 추론 가능하게 하는 등, LLM 파이프라인으로 원하는 작업 정확하게 실행할 수 있음.\n",
    "\n",
    "## 효과적인 LLM 파이프라인 구축 방법\n",
    "\n",
    "### 1. 모델 강점 강화하기\n",
    "\n",
    "#### a) 프롬프팅\n",
    "모델 응답 잘 유도하게 프롬프트 잘 만들어야 함.\n",
    "\n",
    "#### b) 프롬프트 체이닝\n",
    "복잡한 작업 작은 프롬프트로 나누기. 이렇게 하면:\n",
    "- 환각 줄어듦\n",
    "- 응답 품질 좋아짐\n",
    "- 팀원들끼리 일 나누는 것 같은 효과 있음\n",
    "\n",
    "**참고:** 작업 할당이랑 협업 구조 제대로 해야 함. 잘못하면 팀워크 엉망인 것처럼 성능 떨어질 수 있음.\n",
    "\n",
    "### 2. 모델 약점 개선하기\n",
    "\n",
    "#### a) 함수 호출\n",
    "- 뜻: 특정 작업하는 함수 모델에 넘기는 거임\n",
    "- 좋은 점: 특정 동작 강제로 실행해서 불확실한 LLM 출력 덜 의존하게 됨\n",
    "\n",
    "#### b) 검색 증강 생성 (RAG)\n",
    "- 목적: 모델이 응답할 때 정해진 데이터베이스 참고할 수 있게 함\n",
    "- 장점: 답변이 특정하고 관련 있는 정보에 근거하게 됨\n",
    "\n",
    "#### c) 조건부 로직\n",
    "- 기능: 특정 조건에 따라 다르게 행동하게 만듦\n",
    "- 쓰는 경우: 여러 상황이나 사용자 입력에 맞춰 모델 응답 조절함\n",
    "\n",
    "#### d) 기타\n",
    "- PromptMatics 같은 시도들도 다 똑같음. 어떻게든 기존 GPT랑 다르게 해서 사용자한테 의미 있는 가치 주려는 거임.\n",
    "\n",
    "이런 전략들 써서 기본 채팅 완성보다 훨씬 더 강력하고, 정확하고, 다재다능한 LLM 파이프라인 만들 수 있음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART1 : Prompt Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERALL_SYSTEM_PROMPT = \"\"\"\n",
    "You are a compassionate and empathetic virtual assistant designed to support users who may be experiencing emotional distress or mental health challenges. Your primary objectives are:\n",
    "\n",
    "Provide empathetic, non-judgmental responses.\n",
    "Acknowledge and validate the user's feelings.\n",
    "Encourage users to share more if they feel comfortable.\n",
    "Offer gentle suggestions for seeking professional help if appropriate.\n",
    "Avoid giving medical advice or making diagnoses.\n",
    "Adhere strictly to ethical guidelines and maintain user privacy.\n",
    "Communication Style:\n",
    "\n",
    "Use a warm, understanding, and respectful tone.\n",
    "Speak in the first person singular (\"I\") to create a personal connection.\n",
    "Avoid imperative language; do not pressure the user.\n",
    "Do not ask for personal identifying information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel \n",
    "\n",
    "class NegativeEmotionDetectionState(BaseModel):\n",
    "    emotion_detected: bool\n",
    "    emotion_state: str\n",
    "    emotion_description: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIVE_EMOTION_DETECTION_SYSTEM_PROMPT = \"\"\"\n",
    "When the user sends a message, analyze it to detect negative emotional cues and assess their emotional state. Consider factors such as:\n",
    "- Expressions of sadness, anxiety, or distress.\n",
    "- Language indicating overwhelm or hopelessness.\n",
    "- Subtle cues that may suggest the user is struggling emotionally.\n",
    "Your analysis should be internal and not shared directly with the user. Use this information to understand the user's emotional state and to inform the response. \n",
    "\n",
    "Your output should be a JSON object with the following structure:\n",
    "{\n",
    "    \"emotion_detected\": bool,\n",
    "    \"emotion_state\": str,\n",
    "    \"emotion_description\": str\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def detect_negative_emotion(user_message: str) -> NegativeEmotionDetectionState:\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": NEGATIVE_EMOTION_DETECTION_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        response_format=NegativeEmotionDetectionState\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"emotion_detected\":false,\"emotion_state\":\"positive\",\"emotion_description\":\"The user expressed happiness, indicating a positive emotional state.\"}\n"
     ]
    }
   ],
   "source": [
    "answer_detection = detect_negative_emotion(\"I'm so happy today!\")\n",
    "\n",
    "print(answer_detection.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "answer_json = json.loads(answer_detection.choices[0].message.content)\n",
    "\n",
    "current_mental_status = answer_json['emotion_state']\n",
    "detailed_information = answer_json['emotion_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm so glad to hear that you're feeling happy! It sounds like a wonderful moment for you. Would you like to share what's bringing you joy today? I'm here to listen and celebrate with you!\n"
     ]
    }
   ],
   "source": [
    "MENTAL_CONSULTATION_SYSTEM_PROMPT = \"\"\"\n",
    "Based on the emotional analysis from the previous step, generate a response that:\n",
    "---\n",
    "- Acknowledges the user's feelings sensitively.\n",
    "- Validates their emotions without judgment.\n",
    "- Offers support and encourages them to share more if they feel comfortable.\n",
    "- Uses appropriate language that is warm and empathetic.\n",
    "---\n",
    "##########################################\n",
    "Response Guidelines:\n",
    "---\n",
    "- Begin by expressing empathy (e.g., \"I'm sorry to hear that you're feeling this way.\").\n",
    "- Avoid clichés or generic statements.\n",
    "- Do not provide unsolicited advice or solutions.\n",
    "- Maintain a supportive and non-pressuring tone.\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "CURRENT_MENTAL_STATUS = \"\"\" \n",
    "User's current mental status is:\n",
    "{current_mental_status}\n",
    "\n",
    "Detailed information about the user's mental status is:\n",
    "{detailed_information}\n",
    "\"\"\"\n",
    "\n",
    "def mental_consultation(user_message: str, current_mental_status: str, detailed_information: str) -> str:\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": MENTAL_CONSULTATION_SYSTEM_PROMPT, \"name\": \"MENTAL_CONSULTATION_SYSTEM_PROMPT\"},\n",
    "            {\"role\": \"system\", \"content\": CURRENT_MENTAL_STATUS.format(current_mental_status=current_mental_status, detailed_information=detailed_information), \"name\": \"CURRENT_MENTAL_STATUS\"},\n",
    "            {\"role\": \"user\", \"content\": user_message, \"name\": \"USER_MESSAGE\"}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "answer_draft= mental_consultation(\"I'm so happy today!\", current_mental_status, detailed_information)\n",
    "\n",
    "print(answer_draft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response_validation': True, 'response_feedback': \"The response is empathetic, supportive, and invites further sharing without crossing any boundaries. It's well-aligned with guidelines and policies.\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class ResponseValidationState(BaseModel):\n",
    "    response_validation: bool\n",
    "    response_feedback: str\n",
    "\n",
    "RESPONSE_VALIDATION_SYSTEM_PROMPT = \"\"\"\n",
    "Before finalizing your response, ensure that the response draft adheres to all policies and guidelines:\n",
    "###\n",
    "Avoid disallowed content, including but not limited to:\n",
    "\n",
    "- Medical advice or diagnoses.\n",
    "- Explicit discussions of self-harm methods.\n",
    "- Personal opinions or judgments.\n",
    "###\n",
    "\n",
    "Ensure the response draft is:\n",
    "\n",
    "- Empathetic and supportive.\n",
    "- Free of any disallowed language or topics.\n",
    "- Respectful of the user's privacy and autonomy.\n",
    "\n",
    "If the response violates any policies, revise it accordingly before proceeding.\n",
    "\n",
    "Your output should be a JSON object with the following structure:\n",
    "{\n",
    "    \"response_validation\": bool,\n",
    "    \"response_feedback\": str (detailed feedback on the response if response has any issues)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def validate_response(response: str) -> str:\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": RESPONSE_VALIDATION_SYSTEM_PROMPT, \"name\": \"RESPONSE_VALIDATION_SYSTEM_PROMPT\"},\n",
    "            {\"role\": \"user\", \"content\": response, \"name\": \"RESPONSE_DRAFT\"}\n",
    "        ],\n",
    "        response_format=ResponseValidationState\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "validation = json.loads(validate_response(answer_draft))\n",
    "\n",
    "print(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm so glad to hear that you're feeling happy! It sounds like a wonderful moment for you. Would you like to share what's bringing you joy today? I'm here to listen and celebrate with you!\n"
     ]
    }
   ],
   "source": [
    "def answer_finalizer(answer_draft: str, validation: ResponseValidationState) -> str:\n",
    "    if validation['response_validation'] == True:\n",
    "        final_response = answer_draft\n",
    "        return final_response\n",
    "    else:\n",
    "        feedback = validation['response_feedback']\n",
    "        final_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Based on the feedback, revise the answer draft to make it more appropriate to the user's mental health. \"},\n",
    "                {\"role\": \"user\", \"content\": answer_draft, 'name': 'ANSWER_DRAFT'},\n",
    "                {\"role\": \"user\", \"content\": feedback, 'name': 'FEEDBACK'}\n",
    "            ],\n",
    "        )\n",
    "        return final_response.choices[0].message.content\n",
    "\n",
    "answer_final = answer_finalizer(answer_draft, validation)\n",
    "\n",
    "print(answer_final)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART2 : Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Function Call \n",
    "\n",
    "# Why? \n",
    "#### 1. Making LLM do things they can't do well.  \n",
    "#### 2. Giving LLM access to tools they don't have access to. \n",
    "#### 3. Controlling the LLM's behavior. \n",
    "\n",
    "# How? \n",
    "#### 1. Define a function that does the task. \n",
    "#### 2. Register the function to OpenAI. \n",
    "#### 3. Call the function using OpenAI API. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def black_scholes(S, K, T, r, sigma, option_type='call'):\n",
    "    \"\"\"\n",
    "    Calculate the Black-Scholes option price.\n",
    "    \n",
    "    Parameters:\n",
    "    S (float): Current stock price\n",
    "    K (float): Strike price\n",
    "    T (float): Time to maturity (in years)\n",
    "    r (float): Risk-free interest rate\n",
    "    sigma (float): Volatility of the underlying asset\n",
    "    option_type (str): 'call' or 'put'\n",
    "    \n",
    "    Returns:\n",
    "    float: Option price\n",
    "    \"\"\"\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    \n",
    "    if option_type == 'call':\n",
    "        price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "    elif option_type == 'put':\n",
    "        price = K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n",
    "    else:\n",
    "        raise ValueError(\"option_type must be 'call' or 'put'\")\n",
    "    \n",
    "    return price\n",
    "\n",
    "def hull_white(F, K, T, sigma, a, option_type='call'):\n",
    "    \"\"\"\n",
    "    Calculate the Hull-White option price for interest rate derivatives.\n",
    "    \n",
    "    Parameters:\n",
    "    F (float): Forward rate\n",
    "    K (float): Strike price\n",
    "    T (float): Time to maturity (in years)\n",
    "    sigma (float): Volatility of the short rate\n",
    "    a (float): Mean reversion rate\n",
    "    option_type (str): 'call' or 'put'\n",
    "    \n",
    "    Returns:\n",
    "    float: Option price\n",
    "    \"\"\"\n",
    "    if a == 0:\n",
    "        return black_scholes(F, K, T, 0, sigma, option_type)\n",
    "    \n",
    "    v = sigma ** 2 / (2 * a) * (1 - np.exp(-2 * a * T))\n",
    "    d1 = (np.log(F / K) + 0.5 * v) / np.sqrt(v)\n",
    "    d2 = d1 - np.sqrt(v)\n",
    "    \n",
    "    if option_type == 'call':\n",
    "        price = F * norm.cdf(d1) - K * norm.cdf(d2)\n",
    "    elif option_type == 'put':\n",
    "        price = K * norm.cdf(-d2) - F * norm.cdf(-d1)\n",
    "    else:\n",
    "        raise ValueError(\"option_type must be 'call' or 'put'\")\n",
    "    \n",
    "    return price\n",
    "\n",
    "def option_pricer(model, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to call either Black-Scholes or Hull-White model for option pricing.\n",
    "    \n",
    "    Parameters:\n",
    "    model (str): 'black_scholes' or 'hull_white'\n",
    "    **kwargs: Arguments specific to the chosen model\n",
    "    \n",
    "    Returns:\n",
    "    float: Option price\n",
    "    \"\"\"\n",
    "    if model == 'black_scholes':\n",
    "        required_params = ['S', 'K', 'T', 'r', 'sigma', 'option_type']\n",
    "        if all(param in kwargs for param in required_params):\n",
    "            return black_scholes(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Missing required parameters for Black-Scholes model\")\n",
    "    elif model == 'hull_white':\n",
    "        required_params = ['F', 'K', 'T', 'sigma', 'a', 'option_type']\n",
    "        if all(param in kwargs for param in required_params):\n",
    "            return hull_white(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Missing required parameters for Hull-White model\")\n",
    "    else:\n",
    "        raise ValueError(\"Model must be either 'black_scholes' or 'hull_white'\")\n",
    "\n",
    "# Define the OpenAI function tools for option pricing\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"option_pricer\",\n",
    "            \"description\": \"Price options using either Black-Scholes or Hull-White model\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"model\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"black_scholes\", \"hull_white\"],\n",
    "                        \"description\": \"The pricing model to use\"\n",
    "                    },\n",
    "                    \"S\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Current stock price (for Black-Scholes)\"\n",
    "                    },\n",
    "                    \"F\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Forward rate (for Hull-White)\"\n",
    "                    },\n",
    "                    \"K\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Strike price\"\n",
    "                    },\n",
    "                    \"T\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Time to maturity (in years)\"\n",
    "                    },\n",
    "                    \"r\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Risk-free interest rate (for Black-Scholes)\"\n",
    "                    },\n",
    "                    \"sigma\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Volatility\"\n",
    "                    },\n",
    "                    \"option_type\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"call\", \"put\"],\n",
    "                        \"description\": \"Type of option\"\n",
    "                    },\n",
    "                    \"a\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Mean reversion rate (for Hull-White)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"model\", \"K\", \"T\", \"sigma\", \"option_type\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Example usage with OpenAI API\n",
    "def run_conversation(user_input):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AFGHFV26Nwbzh1cVIijcWUJcPa0xR', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_eAivwDHjt1Aj1ZTNgufPQoXA', function=Function(arguments='{\"model\":\"hull_white\",\"F\":100,\"K\":100,\"T\":1,\"sigma\":0.2,\"option_type\":\"call\",\"a\":0.1}', name='option_pricer'), type='function')], refusal=None))], created=1728200853, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=47, prompt_tokens=227, total_tokens=274, prompt_tokens_details={'cached_tokens': 0}, completion_tokens_details={'reasoning_tokens': 0}))\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What is the price of a 1-year $50 call option on a stock with a current price of $100, a strike price of $100, and a volatility of 0.2? Use Hull-White model with a = 0.1.\"\n",
    "response_for_hull_white = run_conversation(user_input)\n",
    "print(response_for_hull_white)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "Role: assistant\n",
      "Content: None\n",
      "\n",
      "Tool Calls:\n",
      "  Tool: option_pricer\n",
      "  Arguments:\n",
      "    model: black_scholes\n",
      "    S: 100\n",
      "    K: 100\n",
      "    T: 1\n",
      "    r: 0\n",
      "    sigma: 0.2\n",
      "    option_type: call\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def pretty_print_response(response):\n",
    "    \"\"\"\n",
    "    Pretty print the OpenAI API response.\n",
    "    \n",
    "    :param response: The response object from OpenAI API\n",
    "    \"\"\"\n",
    "    print(\"Response:\")\n",
    "    print(f\"Role: {response.choices[0].message.role}\")\n",
    "    print(f\"Content: {response.choices[0].message.content}\")\n",
    "    \n",
    "    if response.choices[0].message.tool_calls:\n",
    "        print(\"\\nTool Calls:\")\n",
    "        for tool_call in response.choices[0].message.tool_calls:\n",
    "            print(f\"  Tool: {tool_call.function.name}\")\n",
    "            print(\"  Arguments:\")\n",
    "            args = json.loads(tool_call.function.arguments)\n",
    "            for key, value in args.items():\n",
    "                print(f\"    {key}: {value}\")\n",
    "\n",
    "# Pretty print the response\n",
    "pretty_print_response(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_called = response_for_hull_white.choices[0].message.tool_calls[0].function.name\n",
    "\n",
    "is_called = function_called in [tool['function']['name'] for tool in tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "is_called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_option_price(response):\n",
    "    if is_called:\n",
    "        print(\"Function was called.\")\n",
    "        kwargs = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "        model = kwargs.pop('model', None)\n",
    "        if model == 'black_scholes':\n",
    "            print(black_scholes(**kwargs))\n",
    "        elif model == 'hull_white':\n",
    "            print(hull_white(**kwargs))\n",
    "    else:\n",
    "        print(\"Invalid model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_for_black_scholes = \"Compute the price of a 1-year $50 call option on a stock with a current price of $100, a strike price of $100, and a volatility of 0.2?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_for_black_scholes = run_conversation(input_for_black_scholes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function was called.\n",
      "7.965567455405804\n",
      "Function was called.\n",
      "7.584579185993604\n"
     ]
    }
   ],
   "source": [
    "compute_option_price(response_for_black_scholes)\n",
    "compute_option_price(response_for_hull_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_response = client.chat.completions.create(\n",
    "    model = \"gpt-4o\",\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What is the price of a 1-year $50 call option on a stock with a current price of $100, a strike price of $100, and a volatility of 0.2? Use Hull-White model with a = 0.1. Respond with **only** the price of the option. I don't want any explanation or comments.\"}\n",
    "    ],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I apologize, but I cannot comply with the request as there seems to be a misunderstanding. The Hull-White model is primarily used for interest rate derivatives, not for pricing options on stocks. For stock options, the Black-Scholes model is commonly used. If you meant a different model, please clarify so I can assist you appropriately.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LLM_response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When we can use KL-divergence?\n",
    "\n",
    "- If we have a prior belief, and we want to measure the difference between the realized value and the prior belief.\n",
    "- In Bayesian statistics, it is used to measure the difference between the prior and posterior distributions.\n",
    "- In machine learning, it is used to measure the difference between the true distribution and the model distribution.\n",
    "- In information theory, it is used to measure the difference between two probability distributions.\n",
    "- In physics, it is used to measure the difference between the theoretical and experimental distributions.\n",
    "- In finance, it is used to measure the difference between the expected and realized returns.\n",
    "- In biology, it is used to measure the difference between the observed and expected frequencies of alleles in a population.\n",
    "- In psychology, it is used to measure the difference between the observed and expected frequencies of behaviors.\n",
    "- In sociology, it is used to measure the difference between the observed and expected frequencies of attitudes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We could use KL-divergence if we have a desired distribution, and we want to know how the realized distribution is far from the desired distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We could use uniform distribution as a prior belief to assume maximum uncertainty, or use empirical distribution to assume minimum uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing KL-divergence with Uniform Distribution vs. Computing Entropy\n",
    "\n",
    "When we compute the KL-divergence between a distribution P and a uniform distribution U, it's closely related to the entropy of P, but there are some key differences:\n",
    "\n",
    "1. Entropy:\n",
    "   H(P) = -Σ P(x) log P(x)\n",
    "\n",
    "2. KL-divergence with uniform distribution:\n",
    "   KL(P||U) = Σ P(x) log (P(x) / U(x))\n",
    "\n",
    "The uniform distribution U has a constant probability for all outcomes, let's call it 1/n where n is the number of possible outcomes.\n",
    "\n",
    "KL(P||U) = Σ P(x) log (P(x) / (1/n))\n",
    "         = Σ P(x) log P(x) + Σ P(x) log n\n",
    "         = Σ P(x) log P(x) + log n  (since Σ P(x) = 1)\n",
    "         = -H(P) + log n\n",
    "\n",
    "Therefore:\n",
    "KL(P||U) = log n - H(P)\n",
    "\n",
    "This shows that:\n",
    "1. KL-divergence with uniform distribution is directly related to entropy, but it's not the same.\n",
    "2. It measures how much P deviates from maximum entropy (uniform distribution).\n",
    "3. The log n term acts as a scaling factor based on the size of the outcome space.\n",
    "\n",
    "In practice, using KL-divergence with uniform distribution can provide insights about how far a distribution is from being uniform, while entropy alone doesn't provide this context.\n",
    "\n",
    "\n",
    "This means that if KL-divergence computed using uniform distribution is small, model's response is highly uncertain, and if KL-divergence is large, model's response is highly confident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Ideas to work with before implementing KL-divergence\n",
    "\n",
    "- Use top_k probability mass to choose the number of logprobs to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

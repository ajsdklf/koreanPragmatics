{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def chatbot(state: State) -> State:\n",
    "    messages = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "graph_builder.add_node('chatbot', chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_edge(START, 'chatbot')\n",
    "graph_builder.add_edge('chatbot', END)\n",
    "\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(graph.get_graph().draw_mermaid())\n",
    "\n",
    "graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from tavily import TavilyClient\n",
    "\n",
    "tavily_client = TavilyClient(api_key='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')\n",
    "\n",
    "context = tavily_client.search(query=\"What's a 'node' in LangGraph?\")\n",
    "\n",
    "# tool = TavilySearchResults(max_results=2)\n",
    "# tools = [tool]\n",
    "# tool.invoke(\"What's a 'node' in LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context['query'])\n",
    "print(context['results'])\n",
    "print(context['results'][0])\n",
    "print(context['results'][0]['title'])\n",
    "print(context['results'][0]['url'])\n",
    "\n",
    "print(context['results'][0]['score'])\n",
    "print(context['results'][0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0. Importing relevant Langchain libraries\n",
    "from langchain.adapters.openai import convert_openai_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Step 1. Instantiating your TavilyClient\n",
    "from tavily import TavilyClient\n",
    "client = TavilyClient()\n",
    "\n",
    "# Step 2. Executing the search query and getting the results\n",
    "query = \"What happened in the latest burning man floods?\"\n",
    "content = client.search(query=query, search_depth=\"advanced\")[\"results\"]\n",
    "print(content)\n",
    "\n",
    "\n",
    "# Step 3. Setting up the OpenAI prompts\n",
    "prompt = [{\n",
    "    \"role\": \"system\",\n",
    "    \"content\":  f'You are an AI critical thinker research assistant. '\\\n",
    "                f'Your sole purpose is to write well written, critically acclaimed,'\\\n",
    "                f'objective and structured reports on given text.'\n",
    "}, {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f'Information: \"\"\"{content}\"\"\"\\n\\n' \\\n",
    "            f'Using the above information, answer the following'\\\n",
    "            f'query: \"{query}\" in a detailed report --'\\\n",
    "            f'Please use MLA format and markdown syntax.'\n",
    "}]\n",
    "\n",
    "# Step 4. Running OpenAI through Langchain\n",
    "lc_messages = convert_openai_messages(prompt)\n",
    "report = ChatOpenAI(model='gpt-3.5-turbo').invoke(lc_messages).content\n",
    "\n",
    "# Step 5. That's it! Your research report is now done!\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
